{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56051a1",
   "metadata": {},
   "source": [
    "### Setup stuff\n",
    "#### Install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8498d98",
   "metadata": {},
   "source": [
    "!pip install pytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678c6df",
   "metadata": {
    "time_run": "11:30:30p"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "pytest_path = '/app/data/.local/bin'\n",
    "if pytest_path not in sys.path:\n",
    "    sys.path.append(pytest_path)\n",
    "\n",
    "sys.path\n",
    "\n",
    "import pytest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669e24b",
   "metadata": {},
   "source": [
    "### Ask about anthropic API to get stared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13b899",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "I need to know more about the anthropic API to handle this:\n",
    "The actual assignment: Create a function that takes a prompt and tools, then manually orchestrates the back-and-forth with the Anthropic API until the model stops asking for tool calls. You'll need to handle the JSON parsing, function calling, and message formatting that was being done automatically.\n",
    "\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Great! You're working on implementing tool use with the Anthropic API. This involves creating a loop that handles the conversation flow between your code and the model.\n",
    "\n",
    "Before I suggest an approach, let me understand where you're at:\n",
    "\n",
    "1. Have you already made basic API calls to Anthropic (without tools)?\n",
    "2. Are you familiar with how tool definitions are structured in the API?\n",
    "3. Do you have specific tools/functions you want to use, or are you starting with simple examples?\n",
    "\n",
    "This will help me give you the most useful guidance for your level!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ffa5a6",
   "metadata": {
    "hide_input": true,
    "solveit_ai": true
   },
   "source": [
    "I have no experience with the Anthropic API. The term MCP server was mentioned as though that were common knowledge. I don't know if that is relevant.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "No worries! Let's start from the beginning.\n",
    "\n",
    "The **Anthropic API** lets you send messages to Claude (the AI model) and get responses back. When you add **tools** (also called function calling), you're telling Claude about functions it can use, and Claude can request to call them when helpful.\n",
    "\n",
    "**MCP (Model Context Protocol)** is a newer standard for connecting AI models to tools and data sources. It's useful but not required for your assignment - you can build tool use directly with the Anthropic API.\n",
    "\n",
    "For your assignment, here's what you need to understand first:\n",
    "\n",
    "**The basic flow is:**\n",
    "1. Send a message + tool definitions to Claude\n",
    "2. Claude responds, either with text OR a request to use a tool\n",
    "3. If it requests a tool, you run that function and send the result back\n",
    "4. Repeat until Claude stops requesting tools\n",
    "\n",
    "Have you set up an Anthropic API key yet, or would you like guidance on that first step?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950e1ae5",
   "metadata": {},
   "source": [
    "### Check that ANTHROPIC_API_KEY is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692e2a6c",
   "metadata": {
    "time_run": "11:30:30p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['ANTHROPIC_API_KEY'] is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f7db41",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "The API key is in my environment. Please explain \"Send a message + tool definitions to Claude\". If you can, perhaps you could set up one or more pytests for function(s) that I might need to write.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Perfect! You have the API key set up.\n",
    "\n",
    "**\"Send a message + tool definitions to Claude\"** means you make an API call with:\n",
    "- Your **message** (the user's prompt/question)\n",
    "- **Tool definitions** (JSON descriptions of functions Claude can call - their names, what they do, and what parameters they need)\n",
    "\n",
    "Claude reads both and decides whether to respond with text or request a tool call.\n",
    "\n",
    "For your assignment, I'm thinking you'll need at least these functions:\n",
    "1. One that makes the initial API call with tools\n",
    "2. One that handles the orchestration loop (keeps going until no more tool calls)\n",
    "3. Helper functions to format tool results back into messages\n",
    "\n",
    "Would you like me to write some pytest test cases that show what these functions should do? That way you can write the code to make the tests pass. Or would you prefer I explain more about the API structure first?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b1010",
   "metadata": {},
   "source": [
    "#### Tool definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e85b70a",
   "metadata": {
    "time_run": "11:30:30p"
   },
   "outputs": [],
   "source": [
    "def test_universal_question():\n",
    "    assert universal_question('anything') == 42\n",
    "def universal_question(s: str):\n",
    "    return 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e883b61d",
   "metadata": {
    "time_run": "11:30:30p"
   },
   "outputs": [],
   "source": [
    "test_universal_question()\n",
    "msg = \"Life, the Universe and Everything\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb602bc",
   "metadata": {},
   "source": [
    "#### Code to support message creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5950764",
   "metadata": {
    "time_run": "11:30:30p"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FunctionArg:\n",
    "    name: str\n",
    "    argtype: str\n",
    "    description: str\n",
    "    required: bool = True\n",
    "\n",
    "@dataclass\n",
    "class FunctionName:\n",
    "    name: str\n",
    "    description: str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa643b60",
   "metadata": {
    "time_run": "11:30:30p"
   },
   "outputs": [],
   "source": [
    "def create_function_schema(fname: FunctionName, args: List[FunctionArg]):\n",
    "    properties = {}\n",
    "    required = []\n",
    "    TYPE_MAP = {\n",
    "        str: \"string\",\n",
    "        int: \"integer\",\n",
    "        float: \"number\",\n",
    "        bool: \"boolean\",\n",
    "        list: \"array\",\n",
    "        dict: \"object\"\n",
    "    }    \n",
    "    for arg in args:\n",
    "        at = TYPE_MAP.get(arg.argtype, \"string\")\n",
    "        properties[arg.name] = {\n",
    "            \"type\": at,\n",
    "            \"description\": arg.description\n",
    "        }\n",
    "        if arg.required:\n",
    "            required.append(arg.name)\n",
    "    \n",
    "    return {\n",
    "        \"name\": fname.name,\n",
    "        \"description\": fname.description,\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": properties,\n",
    "            \"required\": required\n",
    "        }\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc32ea4",
   "metadata": {
    "time_run": "11:30:30p"
   },
   "outputs": [],
   "source": [
    "universal_schema = create_function_schema(FunctionName('universal_question', 'Answers for everything'), \n",
    "    [FunctionArg('s', 'str', 'your question', True)])\n",
    "# universal_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80cff5",
   "metadata": {
    "time_run": "11:30:31p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'universal_question',\n",
       " 'description': 'Answers for everything',\n",
       " 'input_schema': {'type': 'object',\n",
       "  'properties': {'s': {'type': 'string', 'description': 'your question'}},\n",
       "  'required': ['s']}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universal_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0c0918",
   "metadata": {},
   "source": [
    "### Initial API call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eea8f1",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "What do you recommend? I want to start with the initial API call.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "I recommend starting with understanding the API structure first, then writing a simple initial call, and *then* adding tests once you see how it works.\n",
    "\n",
    "For the initial API call with tools, you'll use the `anthropic` library. The key parts are:\n",
    "\n",
    "**Basic structure:**\n",
    "```python\n",
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=1024,\n",
    "    tools=[...],  # tool definitions go here\n",
    "    messages=[...]  # conversation history\n",
    ")\n",
    "```\n",
    "\n",
    "The **tools** parameter is a list of dictionaries describing your functions. Each tool needs:\n",
    "- `name`: function name\n",
    "- `description`: what it does\n",
    "- `input_schema`: parameters it accepts (JSON Schema format)\n",
    "\n",
    "Have you installed the `anthropic` Python library yet? If so, would you like to try creating a simple tool definition for a basic function (like adding two numbers), or would you like me to show you an example first?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca25c521",
   "metadata": {},
   "source": [
    "#### Get list of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4df75",
   "metadata": {
    "time_run": "11:30:31p"
   },
   "outputs": [],
   "source": [
    "from claudette import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be55428",
   "metadata": {
    "collapsed": true,
    "time_run": "11:30:31p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['claude-opus-4-1-20250805',\n",
       " 'claude-sonnet-4-5',\n",
       " 'claude-haiku-4-5',\n",
       " 'claude-opus-4-20250514',\n",
       " 'claude-3-opus-20240229',\n",
       " 'claude-sonnet-4-20250514',\n",
       " 'claude-3-7-sonnet-20250219',\n",
       " 'claude-3-5-sonnet-20241022']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1e3656",
   "metadata": {
    "time_run": "11:30:31p"
   },
   "outputs": [],
   "source": [
    "model = 'claude-haiku-4-5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e3b85",
   "metadata": {},
   "source": [
    "#### Function schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b612b71",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "yes\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Great! I see you have `claudette` imported, which is a wrapper around the Anthropic API that can make things easier.\n",
    "\n",
    "You already have a tool schema (`universal_schema`) and a model selected. Now let's make the initial API call.\n",
    "\n",
    "With `claudette`, you can create a client and make a call like this:\n",
    "\n",
    "```python\n",
    "client = Client(model)\n",
    "```\n",
    "\n",
    "Then to call it with your tool:\n",
    "\n",
    "```python\n",
    "response = client(msg, tools=[universal_schema])\n",
    "```\n",
    "\n",
    "Try creating the client and making this call. What response do you get back?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8471c",
   "metadata": {},
   "source": [
    "#### Create anthropic client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a687e",
   "metadata": {
    "time_run": "11:30:31p"
   },
   "outputs": [],
   "source": [
    "import anthropic\n",
    "c = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132e9e1",
   "metadata": {
    "solveit_ai": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5392a2a4",
   "metadata": {
    "solveit_ai": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2022200b",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Please write the client.messages.create() call.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "```python\n",
    "response = c.messages.create(\n",
    "    model=model,\n",
    "    max_tokens=1024,\n",
    "    tools=[universal_schema],\n",
    "    messages=[{\"role\": \"user\", \"content\": msg}]\n",
    ")\n",
    "```\n",
    "\n",
    "Try running this and then look at `response` to see what Claude returns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4556e6c6",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "What is the required form of msg?\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Good question! The `messages` parameter expects a list of message dictionaries. Each message needs:\n",
    "\n",
    "- `\"role\"`: either `\"user\"` or `\"assistant\"` \n",
    "- `\"content\"`: the text of the message\n",
    "\n",
    "So `msg` should be a string (which you already have as `\"Life, the Universe and Everything\"`).\n",
    "\n",
    "The format I showed wraps it: `[{\"role\": \"user\", \"content\": msg}]`\n",
    "\n",
    "Does that clarify it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a71b1c1",
   "metadata": {
    "time_run": "11:30:33p"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[ToolUseBlock(id='toolu_01Pr2LVGruLyQDTwesesFEHf', input={'s': 'Life, the Universe and Everything'}, name='universal_question', type='tool_use')]\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `msg_01QwVH54JfHow9LPe3esZM5M`\n",
       "- content: `[{'id': 'toolu_01Pr2LVGruLyQDTwesesFEHf', 'input': {'s': 'Life, the Universe and Everything'}, 'name': 'universal_question', 'type': 'tool_use'}]`\n",
       "- model: `claude-haiku-4-5-20251001`\n",
       "- role: `assistant`\n",
       "- stop_reason: `tool_use`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 568, 'output_tokens': 59, 'server_tool_use': None, 'service_tier': 'standard'}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_01QwVH54JfHow9LPe3esZM5M', content=[ToolUseBlock(id='toolu_01Pr2LVGruLyQDTwesesFEHf', input={'s': 'Life, the Universe and Everything'}, name='universal_question', type='tool_use')], model='claude-haiku-4-5-20251001', role='assistant', stop_reason='tool_use', stop_sequence=None, type='message', usage=In: 568; Out: 59; Cache create: 0; Cache read: 0; Total Tokens: 627; Search: 0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = c.messages.create(\n",
    "    model=model,\n",
    "    max_tokens=1024,\n",
    "    tools=[universal_schema],\n",
    "    messages=[{\"role\": \"user\", \"content\": msg}]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecffcfe1",
   "metadata": {
    "pinned": true,
    "time_run": "11:30:33p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ToolUseBlock(id='toolu_01Pr2LVGruLyQDTwesesFEHf', input={'s': 'Life, the Universe and Everything'}, name='universal_question', type='tool_use'),\n",
       " 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "response.content[0], len(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89182a0e",
   "metadata": {},
   "source": [
    "Please describe \"Claude responds, either with text OR a request to use a tool\" in more detail for the request to use a tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b65cb",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Perfect! You got a response with a tool use request. Let me break down what Claude returned:\n",
    "\n",
    "**Key parts of the response:**\n",
    "\n",
    "1. **`stop_reason='tool_use'`** - Claude wants to call a tool (not just give text)\n",
    "\n",
    "2. **`content=[ToolUseBlock(...)]`** - Contains the tool call details:\n",
    "   - `name='universal_question'` - which function to call\n",
    "   - `input={'s': 'what is six times 7?'}` - the parameters to pass\n",
    "   - `id='toolu_01BTvVtkCncRT4hvfPLwDifG'` - unique ID for this tool call\n",
    "\n",
    "So Claude is asking: \"Please run `universal_question('what is six times 7?')` and tell me the result.\"\n",
    "\n",
    "Your next step is to:\n",
    "1. Extract the tool name and input\n",
    "2. Call your actual Python function with those parameters\n",
    "3. Send the result back to Claude\n",
    "\n",
    "Does this make sense? What would you get if you called `universal_question('what is six times 7?')`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ee589",
   "metadata": {
    "time_run": "11:30:33p"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[ToolUseBlock(id='toolu_01Pr2LVGruLyQDTwesesFEHf', input={'s': 'Life, the Universe and Everything'}, name='universal_question', type='tool_use')]\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `msg_01QwVH54JfHow9LPe3esZM5M`\n",
       "- content: `[{'id': 'toolu_01Pr2LVGruLyQDTwesesFEHf', 'input': {'s': 'Life, the Universe and Everything'}, 'name': 'universal_question', 'type': 'tool_use'}]`\n",
       "- model: `claude-haiku-4-5-20251001`\n",
       "- role: `assistant`\n",
       "- stop_reason: `tool_use`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 568, 'output_tokens': 59, 'server_tool_use': None, 'service_tier': 'standard'}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_01QwVH54JfHow9LPe3esZM5M', content=[ToolUseBlock(id='toolu_01Pr2LVGruLyQDTwesesFEHf', input={'s': 'Life, the Universe and Everything'}, name='universal_question', type='tool_use')], model='claude-haiku-4-5-20251001', role='assistant', stop_reason='tool_use', stop_sequence=None, type='message', usage=In: 568; Out: 59; Cache create: 0; Cache read: 0; Total Tokens: 627; Search: 0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff451776",
   "metadata": {},
   "source": [
    "#### Handle the tool result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c09369",
   "metadata": {
    "skipped": true,
    "solveit_ai": true
   },
   "source": [
    "What is the API for sending a tool result back to Claude? Please supply an example.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "To send the tool result back, you make another `messages.create()` call with an updated message history that includes:\n",
    "\n",
    "1. The original user message\n",
    "2. Claude's assistant response (with the tool use)\n",
    "3. A new user message with the tool result\n",
    "\n",
    "Here's the structure for the tool result message:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\n",
    "            \"type\": \"tool_result\",\n",
    "            \"tool_use_id\": response.content[0].id,\n",
    "            \"content\": \"42\"  # the result as a string\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "The `tool_use_id` must match the `id` from Claude's tool request.\n",
    "\n",
    "Would you like to try building the full messages list for the next API call?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58902114",
   "metadata": {},
   "source": [
    "#### Send the message to Claude and get a response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a539e",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b4f2b3",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Show the full messages list for the next messages.create() call\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "For the next API call, you need to build up the conversation history with three messages:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": msg},\n",
    "    {\"role\": \"assistant\", \"content\": response.content},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\n",
    "            \"type\": \"tool_result\",\n",
    "            \"tool_use_id\": response.content[0].id,\n",
    "            \"content\": str(42)\n",
    "        }\n",
    "    ]}\n",
    "]\n",
    "```\n",
    "\n",
    "The pattern is: original user message â†’ Claude's tool request â†’ your tool result.\n",
    "\n",
    "Notice the tool result needs the `tool_use_id` from Claude's response. Does this structure make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa00a424",
   "metadata": {},
   "source": [
    "#### Construct new messages list when the stop_reason is tool_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b765a03c",
   "metadata": {
    "time_run": "11:30:33p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Life, the Universe and Everything'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9822501f",
   "metadata": {
    "time_run": "11:30:33p"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[ToolUseBlock(id='toolu_01Pr2LVGruLyQDTwesesFEHf', input={'s': 'Life, the Universe and Everything'}, name='universal_question', type='tool_use')]\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `msg_01QwVH54JfHow9LPe3esZM5M`\n",
       "- content: `[{'id': 'toolu_01Pr2LVGruLyQDTwesesFEHf', 'input': {'s': 'Life, the Universe and Everything'}, 'name': 'universal_question', 'type': 'tool_use'}]`\n",
       "- model: `claude-haiku-4-5-20251001`\n",
       "- role: `assistant`\n",
       "- stop_reason: `tool_use`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 568, 'output_tokens': 59, 'server_tool_use': None, 'service_tier': 'standard'}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_01QwVH54JfHow9LPe3esZM5M', content=[ToolUseBlock(id='toolu_01Pr2LVGruLyQDTwesesFEHf', input={'s': 'Life, the Universe and Everything'}, name='universal_question', type='tool_use')], model='claude-haiku-4-5-20251001', role='assistant', stop_reason='tool_use', stop_sequence=None, type='message', usage=In: 568; Out: 59; Cache create: 0; Cache read: 0; Total Tokens: 627; Search: 0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1116029",
   "metadata": {},
   "source": [
    "#### Send the new list back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25560f82",
   "metadata": {},
   "source": [
    "#### What stop_reasons might we need to handle?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca28646b",
   "metadata": {},
   "source": [
    "**The possible stop_reason values are:**\n",
    "- 'end_turn'\n",
    "- 'max_tokens'\n",
    "- 'stop_sequence'\n",
    "- 'tool_use'\n",
    "- 'pause_turn'\n",
    "- 'refusal'\n",
    "- Plus it can be None (Optional).\n",
    "\n",
    "So we'll want test cases for all 6 (or 7 if you include None). For your orchestration loop, the key ones are still 'tool_use' (continue looping) vs the others (stop).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a660252",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "So this, with a suitably populated tool_schema\n",
    "~~~python\n",
    "response = c.messages.create(\n",
    "    model=model,\n",
    "    max_tokens=1024,\n",
    "    tools=[tool_schema],\n",
    "    messages=messages\n",
    ")\n",
    "~~~\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Yes, exactly! That's the core API call structure you'll use throughout your orchestration loop.\n",
    "\n",
    "The key things that change each iteration are:\n",
    "- **`messages`** - grows with each turn (user â†’ assistant â†’ tool results â†’ assistant...)\n",
    "- **`tools`** - stays the same (your list of available tool schemas)\n",
    "\n",
    "Now you need to wrap this in a loop that:\n",
    "1. Makes the API call\n",
    "2. Checks `response.stop_reason`\n",
    "3. If it's `'tool_use'`, execute the tool and add results to `messages`\n",
    "4. If it's anything else (`'end_turn'`, `'max_tokens'`, etc.), stop and return\n",
    "\n",
    "Do you want to start building that orchestration function, or would you like to discuss the loop structure first?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53e90cd",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "So this, with a suitably populated tool_schema\n",
    "\n",
    "response = c.messages.create(\n",
    "    model=model,\n",
    "    max_tokens=1024,\n",
    "    tools=[tool_schema],\n",
    "    messages=messages\n",
    ")\n",
    "seems to be good for both the initial call and a subsequent invocation to use a tool, with two additional messages in the list. The remaining question is do all calls except the first always append 2 more messages in the same format that we have seen, except when the stop_reason is 'end_turn'. If not, then we may need a function that is dispatched to construct the new appended messages based on the other stop reasons.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Great analysis! You're thinking about this systematically.\n",
    "\n",
    "For the orchestration loop, here's what happens with different `stop_reason` values:\n",
    "\n",
    "**`'tool_use'`** - Yes, append 2 messages:\n",
    "1. Assistant's response (with tool use blocks)\n",
    "2. User's tool results\n",
    "\n",
    "**`'end_turn'`** - Stop, no more messages needed\n",
    "\n",
    "**Other stop reasons** (`'max_tokens'`, `'stop_sequence'`, etc.) - Also stop the loop\n",
    "\n",
    "So you really only need to handle the `'tool_use'` case for continuing. The others all mean \"we're done.\"\n",
    "\n",
    "One edge case: Claude might request **multiple tools** in a single response. Have you checked if `response.content` can have more than one `ToolUseBlock`? \n",
    "\n",
    "Would you like to explore that, or start sketching the orchestration function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc0e22",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "content: [{'citations': None, 'text': 'The answer is **42**.\\n\\nThis is, of course, the famous answer from Douglas Adams\\' \"The Hitchhiker\\'s Guide to the Galaxy,\" where a supercomputer named Deep Thought spends 7.5 million years calculating the Answer to the Ultimate Question of Life, the Universe, and Everythingâ€”only to reveal that the answer is simply **42**. \\n\\nThe joke being that while we finally have the answer, nobody really knows what the actual question is! It\\'s become an iconic reference in science fiction, mathematics, and pop culture.', 'type': 'text'}]\n",
    "\n",
    "It looks like content could return a list of length>1, maybe even an empty list.\n",
    "\n",
    "I wonder whether 'type' might return something other than 'text'. Or what citations might be, for that matter. It looks like almost anything *could* happen. Is there a guarantee, for example, that all elements of the content list are dict?\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Excellent observation! You're right to be cautious about assumptions.\n",
    "\n",
    "From the API, `content` is a list that can contain different block types:\n",
    "- **`TextBlock`** - has `type='text'` and `text` field\n",
    "- **`ToolUseBlock`** - has `type='tool_use'`, `name`, `input`, and `id`\n",
    "\n",
    "You've already seen both! Look back at your earlier responses:\n",
    "- When `stop_reason='tool_use'`: content had `ToolUseBlock` objects\n",
    "- When `stop_reason='end_turn'`: content had `TextBlock` objects\n",
    "\n",
    "They're not raw dicts - they're objects with attributes. That's why you used `response.content[0].id` and `response.content[0].text`.\n",
    "\n",
    "For your orchestration, you need to:\n",
    "1. Check if each block in `content` is a `ToolUseBlock` (type checking or check `type` attribute)\n",
    "2. Extract tool info from those blocks\n",
    "3. Ignore `TextBlock` objects when processing tool calls\n",
    "\n",
    "Does this clarify the structure? Want to try writing code to filter for just the tool use blocks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970bb216",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Please list all possible stop_reasons\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "The possible `stop_reason` values from the Anthropic API are:\n",
    "\n",
    "- **`'end_turn'`** - Claude finished its response naturally (no more tool calls needed)\n",
    "- **`'tool_use'`** - Claude wants to call a tool\n",
    "- **`'max_tokens'`** - Hit the token limit you set\n",
    "- **`'stop_sequence'`** - Encountered a custom stop sequence (if you defined one)\n",
    "\n",
    "For your orchestration loop, you mainly care about checking if `stop_reason == 'tool_use'` to know whether to continue the loop or stop.\n",
    "\n",
    "Does that help clarify when to keep going vs when to finish?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ddfaa",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Ok. Let's start cleaing this up. Start with a function that returns the stop_reason from the response. Let's write this and a corresponding pytest for it.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Good idea to start organizing the code!\n",
    "\n",
    "Let's start with the test first. What should we call this function? Something like `get_stop_reason`?\n",
    "\n",
    "And for the test, what would you expect it to return when given your `response` object?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd11f0e",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "get_stop_reason is good enough. We should have at least one pytest for each possible stop reason.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Great! So you'll need 4 test cases, one for each stop reason.\n",
    "\n",
    "For the tests, you'll need to create mock response objects. You could either:\n",
    "1. Create simple objects with a `stop_reason` attribute\n",
    "2. Use actual `Message` objects (more realistic but more setup)\n",
    "\n",
    "Which approach would you prefer? The simple object approach would look something like:\n",
    "\n",
    "```python\n",
    "class MockResponse:\n",
    "    def __init__(self, stop_reason):\n",
    "        self.stop_reason = stop_reason\n",
    "```\n",
    "\n",
    "What do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158e6909",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "We need response objects, not message objects for the get_stop_reason function.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "You're right - I should have said response objects, not message objects. The `response` you get back from `c.messages.create()` is a `Message` object (which is what Anthropic calls their response type).\n",
    "\n",
    "So for your tests, do you want to:\n",
    "1. Use the simple `MockResponse` class I suggested\n",
    "2. Create actual Anthropic `Message` objects\n",
    "\n",
    "Which would you prefer to work with?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4951d4",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Ah. Ok. So yes, then we want to create Anthropic Message objects for testing. We might want to just start with parametrized tests (pytest.mark.parametrize). So the results and messages can be set up in a list.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Perfect! Parametrized tests are a clean approach here.\n",
    "\n",
    "For creating Anthropic `Message` objects, you'll need to provide the required fields. Looking at your earlier response, the key fields are:\n",
    "- `id`\n",
    "- `content` \n",
    "- `model`\n",
    "- `role`\n",
    "- `stop_reason`\n",
    "- `type`\n",
    "- `usage`\n",
    "\n",
    "You could create a helper function that builds a minimal `Message` object with just the `stop_reason` varying. \n",
    "\n",
    "Would you like to try writing the parametrized test structure, or would you like me to show you an example of how to construct a `Message` object first?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb1a86",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Let's go with a minimal helper function to start. We can expand on that as needed.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Good plan! Try creating a helper function that takes a `stop_reason` parameter and returns a `Message` object with minimal required fields.\n",
    "\n",
    "Start with something like:\n",
    "\n",
    "```python\n",
    "def create_test_message(stop_reason):\n",
    "    return anthropic.types.Message(...)\n",
    "```\n",
    "\n",
    "Try filling in the fields based on what you saw in your earlier response. What fields do you think are absolutely required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868a6077",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "I am not that familiar with the structure and its usage. It looks like we may want content and stop_reason. I have only seen two roles, so far, user and assistant. model seems irrelevant for testing purposes.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "You're right on track! For testing `get_stop_reason`, you really only need `stop_reason` itself. But to create a valid `Message` object, you might need some other required fields.\n",
    "\n",
    "Try this approach: look at the `anthropic.types.Message` to see what's required. You can check with:\n",
    "\n",
    "```python\n",
    "import inspect\n",
    "inspect.signature(anthropic.types.Message)\n",
    "```\n",
    "\n",
    "Or just try creating a minimal one and see what error you get. That will tell you which fields are actually required!\n",
    "\n",
    "Give it a shot - what happens when you try to create a `Message` with just `stop_reason`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3fd7ef",
   "metadata": {
    "time_run": "11:30:33p"
   },
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a71860",
   "metadata": {
    "time_run": "11:30:33p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: <class 'str'>\n",
      "\n",
      "content: typing.List[typing.Annotated[typing.Union[anthropic.types.text_block.TextBlock, anthropic.types.thinking_block.ThinkingBlock, anthropic.types.redacted_thinking_block.RedactedThinkingBlock, anthropic.types.tool_use_block.ToolUseBlock, anthropic.types.server_tool_use_block.ServerToolUseBlock, anthropic.types.web_search_tool_result_block.WebSearchToolResultBlock], PropertyInfo(alias='None', format=None, format_template='None', discriminator='type')]]\n",
      "\n",
      "model: typing.Union[typing.Literal['claude-3-7-sonnet-latest', 'claude-3-7-sonnet-20250219', 'claude-3-5-haiku-latest', 'claude-3-5-haiku-20241022', 'claude-haiku-4-5', 'claude-haiku-4-5-20251001', 'claude-sonnet-4-20250514', 'claude-sonnet-4-0', 'claude-4-sonnet-20250514', 'claude-sonnet-4-5', 'claude-sonnet-4-5-20250929', 'claude-opus-4-0', 'claude-opus-4-20250514', 'claude-4-opus-20250514', 'claude-opus-4-1-20250805', 'claude-3-opus-latest', 'claude-3-opus-20240229', 'claude-3-haiku-20240307'], str]\n",
      "\n",
      "role: typing.Literal['assistant']\n",
      "\n",
      "stop_reason: typing.Optional[typing.Literal['end_turn', 'max_tokens', 'stop_sequence', 'tool_use', 'pause_turn', 'refusal']]\n",
      "\n",
      "stop_sequence: typing.Optional[str]\n",
      "\n",
      "type: typing.Literal['message']\n",
      "\n",
      "usage: <class 'anthropic.types.usage.Usage'>\n",
      "\n",
      "extra_data: typing.Any\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sig = inspect.signature(anthropic.types.Message)\n",
    "for param_name, param in sig.parameters.items():\n",
    "    print(f\"{param_name}: {param.annotation}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21450d2",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Hmm. Before you listed 4 stop reasons. Now I see that it looks like there are 6 possibilities.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "You're absolutely right! I apologize for the incomplete list earlier. Looking at the signature, the possible `stop_reason` values are:\n",
    "\n",
    "1. `'end_turn'`\n",
    "2. `'max_tokens'`\n",
    "3. `'stop_sequence'`\n",
    "4. `'tool_use'`\n",
    "5. `'pause_turn'`\n",
    "6. `'refusal'`\n",
    "\n",
    "Plus it can be `None` (Optional).\n",
    "\n",
    "So you'll want test cases for all 6 (or 7 if you include `None`). For your orchestration loop, the key ones are still `'tool_use'` (continue looping) vs the others (stop).\n",
    "\n",
    "Does this change how you want to approach the parametrized tests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c119f12",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Ugh. I hit shift-enter instead of enter, but that answer helps. the haiku model can be a default.\n",
    "role - I thought that started as user, does it transition only to assistant?\n",
    "I will inspect usage next.\n",
    "\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Good observations!\n",
    "\n",
    "For **role**: In the `Message` response object from the API, the role is always `'assistant'` because it's Claude's response. The `'user'` role appears in the `messages` list you send *to* the API (the conversation history).\n",
    "\n",
    "So for your test `Message` objects, role will always be `'assistant'`.\n",
    "\n",
    "Go ahead and inspect `usage` - let me know what you find!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc019c",
   "metadata": {
    "time_run": "11:30:33p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache_creation: typing.Optional[anthropic.types.cache_creation.CacheCreation]\n",
      "\n",
      "cache_creation_input_tokens: typing.Optional[int]\n",
      "\n",
      "cache_read_input_tokens: typing.Optional[int]\n",
      "\n",
      "input_tokens: <class 'int'>\n",
      "\n",
      "output_tokens: <class 'int'>\n",
      "\n",
      "server_tool_use: typing.Optional[anthropic.types.server_tool_usage.ServerToolUsage]\n",
      "\n",
      "service_tier: typing.Optional[typing.Literal['standard', 'priority', 'batch']]\n",
      "\n",
      "extra_data: typing.Any\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usg = inspect.signature(anthropic.types.usage.Usage)\n",
    "for param_name, param in usg.parameters.items():\n",
    "    print(f\"{param_name}: {param.annotation}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0168cd42",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "most of usage looks like state maintenance, with service_tier probably user selectable. I think we won't need this for testing.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "You're right that most of `usage` is just tracking/billing info. However, looking at the signature, `input_tokens` and `output_tokens` are **required** (not Optional), so you'll need to provide them when creating a test `Message`.\n",
    "\n",
    "You can just use dummy values like `input_tokens=0, output_tokens=0` for testing.\n",
    "\n",
    "So for your helper function, the required fields are:\n",
    "- `id` (str)\n",
    "- `content` (list)\n",
    "- `model` (str)\n",
    "- `role` ('assistant')\n",
    "- `type` ('message')\n",
    "- `usage` (Usage object with input_tokens and output_tokens)\n",
    "- `stop_reason` (your test parameter)\n",
    "\n",
    "Want to try creating the helper function now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b0d4c",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Yes. Show me what you think.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Here's a helper function to create test messages:\n",
    "\n",
    "```python\n",
    "def create_test_message(stop_reason):\n",
    "    return anthropic.types.Message(\n",
    "        id='test_id',\n",
    "        content=[],\n",
    "        model='claude-haiku-4-5',\n",
    "        role='assistant',\n",
    "        type='message',\n",
    "        usage=anthropic.types.Usage(input_tokens=0, output_tokens=0),\n",
    "        stop_reason=stop_reason\n",
    "    )\n",
    "```\n",
    "\n",
    "This gives you a minimal valid `Message` object with just the stop_reason varying. Does this look good to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab699db9",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Beautiful. Let's set up the 6 stop reasons and also add one for None\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Great! Now try setting up the parametrized test. You'll want something like:\n",
    "\n",
    "```python\n",
    "@pytest.mark.parametrize(\"stop_reason\", [\n",
    "    'end_turn',\n",
    "    'max_tokens',\n",
    "    # add the rest...\n",
    "])\n",
    "def test_get_stop_reason(stop_reason):\n",
    "    # your test here\n",
    "```\n",
    "\n",
    "Can you fill in the remaining stop reasons and write the test body?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96395a3f",
   "metadata": {
    "time_run": "11:30:34p"
   },
   "outputs": [],
   "source": [
    "def create_test_message(stop_reason):\n",
    "    return anthropic.types.Message(\n",
    "        id='test_id',\n",
    "        content=[],\n",
    "        model='claude-haiku-4-5',\n",
    "        role='assistant',\n",
    "        type='message',\n",
    "        usage=anthropic.types.Usage(input_tokens=0, output_tokens=0),\n",
    "        stop_reason=stop_reason\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f6dfa1",
   "metadata": {
    "time_run": "11:30:34p"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "None\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `test_id`\n",
       "- content: `[]`\n",
       "- model: `claude-haiku-4-5`\n",
       "- role: `assistant`\n",
       "- stop_reason: `tool_use`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': None, 'cache_creation_input_tokens': None, 'cache_read_input_tokens': None, 'input_tokens': 0, 'output_tokens': 0, 'server_tool_use': None, 'service_tier': None}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='test_id', content=[], model='claude-haiku-4-5', role='assistant', stop_reason='tool_use', stop_sequence=None, type='message', usage=In: 0; Out: 0; Cache create: 0; Cache read: 0; Total Tokens: 0; Search: 0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_test_message('tool_use')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6345144e",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Create a parametrized test for the end_turn case\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Here's a parametrized test for the `end_turn` case:\n",
    "\n",
    "```python\n",
    "@pytest.mark.parametrize(\"stop_reason\", [\"end_turn\"])\n",
    "def test_get_stop_reason(stop_reason):\n",
    "    msg = create_test_message(stop_reason)\n",
    "    assert get_stop_reason(msg) == stop_reason\n",
    "```\n",
    "\n",
    "Now you can add the other stop reasons to the list. Want to try expanding the parameter list to include all 6 stop reasons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d1205f",
   "metadata": {
    "time_run": "11:30:34p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d3eb6",
   "metadata": {
    "time_run": "11:30:34p"
   },
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    'end_turn',\n",
    "    'max_tokens',\n",
    "    'stop_sequence',\n",
    "    'tool_use',\n",
    "    'pause_turn',\n",
    "    'refusal',\n",
    "]\n",
    "\n",
    "@pytest.mark.parametrize(\"a\", test_cases)\n",
    "def test_get_stop_reason(a):\n",
    "    msg = create_test_message(a)\n",
    "    assert get_stop_reason(msg) == a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe7d532",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "we never wrote the get_stop_reason() function. Also the message has a stop_sequence field, I think, not stop_reason.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "You're right on both counts!\n",
    "\n",
    "1. You need to write `get_stop_reason()` - it should be simple, just return the stop_reason from the message\n",
    "2. Looking at your earlier code, you accidentally set `stop_sequence=stop_reason` instead of `stop_reason=stop_reason` in `create_test_message`\n",
    "\n",
    "Fix the `create_test_message` function first (change `stop_sequence` to `stop_reason`), then write the `get_stop_reason` function. Give it a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f713df",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "haha. Again. Please give me a get_stop_reason function taking a message as an argument. Remember that we need to return the stop_sequence field from the message.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Wait - I need to clarify something. Looking at your earlier actual API response, the field is called `stop_reason`, not `stop_sequence`. \n",
    "\n",
    "The `stop_sequence` field is different - it holds the actual stop sequence text if one was encountered.\n",
    "\n",
    "So `get_stop_reason` should return `message.stop_reason`. Does that make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75335458",
   "metadata": {
    "time_run": "11:30:34p"
   },
   "outputs": [],
   "source": [
    "def get_stop_reason(msg):\n",
    "    return msg.stop_reason"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879359ae",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Is it possible to move cells up or down in the notebook?\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Yes! In this Dialog Engineering environment, you can reorder cells by dragging them. Look for a handle or grip area on the left side of each message/cell - you should be able to click and drag to move cells up or down.\n",
    "\n",
    "Are you able to see the drag handles on your cells?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35955d",
   "metadata": {
    "time_run": "11:30:34p"
   },
   "outputs": [],
   "source": [
    "def test_get_stop_reason():\n",
    "    msg = create_test_message('tool_use')\n",
    "    assert get_stop_reason(msg) == 'tool_use'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78cc82",
   "metadata": {
    "time_run": "11:30:34p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "473d0833",
   "metadata": {
    "solveit_ai": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf9e083c",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "OMG. I am at the bottom of the notebook again. Organizing cells is painful.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "I hear you - that does sound frustrating! Scrolling back and forth can definitely break your flow.\n",
    "\n",
    "A few strategies that might help:\n",
    "- Use the \"Run all\" button to execute all code cells from top to bottom when you restart\n",
    "- Keep related code together as you write it\n",
    "- Use the note cells to create section headers so you can quickly scan and find things\n",
    "\n",
    "Are you looking to reorganize something specific right now, or do you want to continue building the orchestration function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badfe938",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "I was in the middle of reorganizing by moving to the top, but some evil magic pushed my continued prompt down to the bottom. I think the fundamental problem stems from only being able to input keystrokes AT THE BOTTOM OF THE PAGE, so you don't notice that you \"aren't in Kansas\" anymore - i.e. the cell you are modifying is not \"geographically\" connected to your input.\n",
    "This is a real UX issue. \n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "You're absolutely right - that's a significant UX issue. Having the input area fixed at the bottom while the cell you're editing might be anywhere in the notebook creates a disconnect that makes it easy to lose context and get disoriented. It's especially problematic when cells auto-scroll or move unexpectedly.\n",
    "\n",
    "I can see how frustrating that would be, especially when trying to organize and refactor your code. Unfortunately, I can't change the interface behavior, but your feedback is valid.\n",
    "\n",
    "Do you want to take a break from reorganizing and continue with the orchestration logic, or would you prefer to work on getting things organized first?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a8d56b",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Can you tell me where I was before this? Which Note cell was closest?\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Looking at your context, the most recent note cell before your UX frustration was:\n",
    "\n",
    "**\"##### What stop_reasons might we need to handle?\"**\n",
    "\n",
    "That was right after you successfully completed the second API call and got the response with `stop_reason='end_turn'`.\n",
    "\n",
    "Does that help you get reoriented?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a030aad",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "before this prompt:\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Before your prompt about the UX issue, you were discussing the structure of tool definitions. Specifically, you had just made this observation:\n",
    "\n",
    "**\"properties looks to be argument(s) to the function, with each argument name having a type and a description. finally, there is a list of the names of required arguments. Presumably any other arguments just won't appear in the list.\"**\n",
    "\n",
    "And I confirmed you were correct about how the tool schema works - `properties` defines all parameters, and `required` lists which ones are mandatory.\n",
    "\n",
    "Does that help you pick up where you left off?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6157693",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Right. What is the closest NOTE to the prompt chronologically before that \"properties\" prompt?\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Looking at the chronology, the closest NOTE cell before your \"properties\" prompt was:\n",
    "\n",
    "**\"#### Create a function\"**\n",
    "\n",
    "This was right before you created the `test_universal_question()` and `universal_question()` functions.\n",
    "\n",
    "Is that the section you're trying to get back to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6d366",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "ok let's flesh out create_function. We will need to loop through the args to create the input schema and also build a required list.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Good plan! Let's build `create_function` step by step.\n",
    "\n",
    "It should:\n",
    "1. Take a `FunctionName` and list of `FunctionArg` objects\n",
    "2. Loop through args to build the `properties` dict\n",
    "3. Build the `required` list from args where `required=True`\n",
    "4. Return the complete tool schema dict\n",
    "\n",
    "Want to try writing the loop that builds the `properties` dict first? Each property needs the arg name as the key, and a dict with `type` and `description` as the value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9122b63d",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "plan looks good. DoIt.\n",
    "\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Here's the `create_function` implementation:\n",
    "\n",
    "```python\n",
    "def create_function(fname: FunctionName, args: List[FunctionArg]):\n",
    "    properties = {}\n",
    "    required = []\n",
    "    \n",
    "    for arg in args:\n",
    "        properties[arg.name] = {\n",
    "            \"type\": arg.argtype,\n",
    "            \"description\": arg.description\n",
    "        }\n",
    "        if arg.required:\n",
    "            required.append(arg.name)\n",
    "    \n",
    "    return {\n",
    "        \"name\": fname.name,\n",
    "        \"description\": fname.description,\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": properties,\n",
    "            \"required\": required\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "Try it out with your `universal_question` function!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1e723b",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "I would like to add the mapping. Show me a nice concise implementation.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Here's a concise type mapping:\n",
    "\n",
    "```python\n",
    "TYPE_MAP = {\n",
    "    str: \"string\",\n",
    "    int: \"integer\",\n",
    "    float: \"number\",\n",
    "    bool: \"boolean\",\n",
    "    list: \"array\",\n",
    "    dict: \"object\"\n",
    "}\n",
    "```\n",
    "\n",
    "Then in your `FunctionArg`, use: `argtype = TYPE_MAP.get(argtype, argtype)` to convert Python types to JSON Schema types, or pass through if already a string.\n",
    "\n",
    "Want to integrate this into your existing code?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc3952",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "yes\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Where would you like to add the type mapping? In the `function_entry` function, in `create_function`, or both?\n",
    "\n",
    "Also, should it handle cases where someone passes in a string like `\"string\"` (already in JSON Schema format) vs a Python type like `str`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681404f1",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "I think I will be using the two dataclasses. \n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Good! So you want the type mapping to work when creating `FunctionArg` objects.\n",
    "\n",
    "You could either:\n",
    "1. Add the mapping logic inside `function_entry` so it automatically converts Python types\n",
    "2. Keep `function_entry` simple and let users pass either Python types or JSON Schema strings\n",
    "\n",
    "Which approach do you prefer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7fffc",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "So we can terminate the looping when the stop_reason != 'tool_use'. We have not spoken, as far as I remember, on what a toolloop functon would return. I suppose it could just return the last response. Is there any reason why it might be useful to return an list of responses? Is there any other data that we might want that was created only within toolloop?\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Great question! Let's think about what might be useful:\n",
    "\n",
    "**Just returning the last response:**\n",
    "- Simple and clean\n",
    "- Has the final text answer\n",
    "- But you lose the conversation history\n",
    "\n",
    "**Returning a list of all responses:**\n",
    "- Shows the full chain of reasoning\n",
    "- Useful for debugging\n",
    "- Can see what tools were called\n",
    "\n",
    "**Returning the messages list:**\n",
    "- Contains the complete conversation\n",
    "- Can be used to continue the conversation later\n",
    "- Shows both tool calls and results\n",
    "\n",
    "What do you think would be most useful for your use case? Do you need to see the intermediate steps, or just the final answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e95aa",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "ok. I'll go Riesian. My use case is completing a homework assignment. I'll return the final response, even though I might only need the text.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Perfect! Returning the final response is clean and practical - you get the text plus any metadata you might need later.\n",
    "\n",
    "So your `toolloop` function will:\n",
    "1. Take initial prompt and tools\n",
    "2. Loop while `stop_reason == 'tool_use'`\n",
    "3. Execute tools and add results to messages\n",
    "4. Return the final response when done\n",
    "\n",
    "Want to start sketching out the function signature and main loop structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387eb66e",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Sure. I actually wanted the function signature from the outset. But now is a good time, since we have refined what we want out of the loop.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Great! What parameters do you think `toolloop` needs?\n",
    "\n",
    "Looking at what you've done so far, I'm thinking:\n",
    "- The user's initial message/prompt\n",
    "- The list of tool schemas\n",
    "- Maybe the model name?\n",
    "\n",
    "What do you think should go in the signature?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df6657",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Right now access to the function has used a python globals() hack. That seems awkward - but maybe good enough for homework. It might make more sense to \"register\" functions when they are created, so that they can be dispatched by name. There is no reason not to include the model name in the loop signature. It costs nothing to do it there. It gains immediate functionality if someone wants to do model comparison. Or selection based on cost.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Good thinking! You're weighing tradeoffs nicely.\n",
    "\n",
    "For the function dispatch, you could:\n",
    "1. Keep the `globals()` hack (simple, works for homework)\n",
    "2. Pass a dict mapping function names to actual functions\n",
    "3. Create a registry that `create_function` automatically populates\n",
    "\n",
    "The dict approach is clean: `tools_dict = {'universal_question': universal_question}` and pass that in.\n",
    "\n",
    "So your signature might be:\n",
    "```python\n",
    "def toolloop(prompt: str, tools: list, tools_dict: dict, model: str):\n",
    "```\n",
    "\n",
    "Does that feel right, or do you want to adjust it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff9209",
   "metadata": {
    "time_run": "11:30:34p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44745a34",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "def toolloop(prompt: str, tools: list, tools_dict: dict, model: str) -> anthropic.types.message.Message): \n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Perfect signature! Now you have:\n",
    "- `prompt`: the user's question\n",
    "- `tools`: list of tool schemas\n",
    "- `tools_dict`: mapping of function names to actual functions\n",
    "- `model`: which Claude model to use\n",
    "- Returns: the final `Message` object\n",
    "\n",
    "Want to start writing the body? What should be the first step inside the function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4e4737",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "do we want to construct the client inside the function or pass it in?\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Good question! Consider:\n",
    "\n",
    "**Create client inside:**\n",
    "- Self-contained function\n",
    "- But creates a new client each call (minor overhead)\n",
    "\n",
    "**Pass client in:**\n",
    "- Reuse existing client\n",
    "- More flexible (easier to mock for testing)\n",
    "- Caller controls the client configuration\n",
    "\n",
    "Which feels better for your use case? You already have `c = anthropic.Anthropic()` created - would you want to reuse that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab08b84c",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "pass client in\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Good choice! So your updated signature would be:\n",
    "\n",
    "```python\n",
    "def toolloop(client: anthropic.Anthropic, prompt: str, tools: list, tools_dict: dict, model: str) -> anthropic.types.message.Message:\n",
    "```\n",
    "\n",
    "Now, what should be the first thing you do inside the function? Think about what you need to set up before entering the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69233228",
   "metadata": {
    "time_run": "11:30:34p"
   },
   "outputs": [],
   "source": [
    "def toolloop(client: anthropic.Anthropic, prompt: str, tools: list, tools_dict: dict, model: str) -> anthropic.types.message.Message:\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=1024,\n",
    "        tools=tools,\n",
    "        messages=messages\n",
    "    )\n",
    "    while response.stop_reason == 'tool_use':\n",
    "        # tool_block = response.content[0]\n",
    "        # func = tools_dict[tool_block.name]\n",
    "        # result = func(**tool_block.input)\n",
    "\n",
    "        messages.append(dict(role=response.role, content=response.content))\n",
    "        \n",
    "        contents = []\n",
    "        for block in response.content:\n",
    "            if block.type == 'tool_use':\n",
    "                func = tools_dict[block.name]\n",
    "                result = func(**block.input)\n",
    "                contents.append(dict(\n",
    "                    type=\"tool_result\",\n",
    "                    tool_use_id=block.id,\n",
    "                    content=str(result)\n",
    "                ))\n",
    "        messages.append(dict(role=\"user\", content=contents))\n",
    "        \n",
    "        response = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=1024,\n",
    "            tools=tools,\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b95f86d",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Now?\n",
    "\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Excellent! You've got a complete `toolloop` implementation that handles:\n",
    "- Initial API call\n",
    "- Looping on `tool_use`\n",
    "- **Multiple tool calls** in a single response (good catch with the loop!)\n",
    "- Building tool results\n",
    "- Final response return\n",
    "\n",
    "This looks solid! Have you tested it with your `universal_question` example? Try:\n",
    "\n",
    "```python\n",
    "result = toolloop(c, msg, [universal_schema], {'universal_question': universal_question}, model)\n",
    "```\n",
    "\n",
    "What do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69fc207",
   "metadata": {
    "time_run": "11:30:37p"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The answer is **42**.\n",
       "\n",
       "This is, of course, a reference to Douglas Adams' famous science fiction comedy series *The Hitchhiker's Guide to the Galaxy*, where a supercomputer named Deep Thought spends 7.5 million years calculating the answer to the Ultimate Question of Life, the Universe, and Everything, only to reveal the answer is simply: **42**.\n",
       "\n",
       "The joke is that while we finally have the answer, the actual question remains a profound mysteryâ€”a clever commentary on the human search for meaning and the ultimate absurdity of existence! ðŸ˜„\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `msg_01WK2uyHVTJ3y3LTNoEnTX4r`\n",
       "- content: `[{'citations': None, 'text': \"The answer is **42**.\\n\\nThis is, of course, a reference to Douglas Adams' famous science fiction comedy series *The Hitchhiker's Guide to the Galaxy*, where a supercomputer named Deep Thought spends 7.5 million years calculating the answer to the Ultimate Question of Life, the Universe, and Everything, only to reveal the answer is simply: **42**.\\n\\nThe joke is that while we finally have the answer, the actual question remains a profound mysteryâ€”a clever commentary on the human search for meaning and the ultimate absurdity of existence! ðŸ˜„\", 'type': 'text'}]`\n",
       "- model: `claude-haiku-4-5-20251001`\n",
       "- role: `assistant`\n",
       "- stop_reason: `end_turn`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 640, 'output_tokens': 125, 'server_tool_use': None, 'service_tier': 'standard'}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_01WK2uyHVTJ3y3LTNoEnTX4r', content=[TextBlock(citations=None, text=\"The answer is **42**.\\n\\nThis is, of course, a reference to Douglas Adams' famous science fiction comedy series *The Hitchhiker's Guide to the Galaxy*, where a supercomputer named Deep Thought spends 7.5 million years calculating the answer to the Ultimate Question of Life, the Universe, and Everything, only to reveal the answer is simply: **42**.\\n\\nThe joke is that while we finally have the answer, the actual question remains a profound mysteryâ€”a clever commentary on the human search for meaning and the ultimate absurdity of existence! ðŸ˜„\", type='text')], model='claude-haiku-4-5-20251001', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 640; Out: 125; Cache create: 0; Cache read: 0; Total Tokens: 765; Search: 0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = toolloop(c, msg, [universal_schema], {'universal_question': universal_question}, model)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9efe133",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Looks good. Thanks. :)\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "You're welcome! Great job working through this! You now have a complete tool orchestration system that:\n",
    "- Handles tool definitions\n",
    "- Makes API calls with tools\n",
    "- Loops through multiple tool use rounds\n",
    "- Returns the final response\n",
    "\n",
    "Is there anything else you'd like to add or test, or are you good with your homework assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6c3a6",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "I guess I would like to restart and run all cells. Does that work from the two icons up next to where the Terminal icon is?\n",
    "\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Yes! The icons near the Terminal should include:\n",
    "- **Clear** or **Restart** - restarts the Python interpreter (clears all variables)\n",
    "- **Run All** - executes all code cells from top to bottom\n",
    "\n",
    "If you want to test everything fresh, you'd typically:\n",
    "1. Click **Clear** to restart the interpreter\n",
    "2. Click **Run All** to execute all your code cells in order\n",
    "\n",
    "This will verify that all your code works when run sequentially. Give it a try and let me know if anything breaks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9b7111",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Clear is probably what I want. I want no longer used variables to disapper. Which one is Clear?\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "**Clear** is the one that restarts the Python interpreter and removes all variables. It's typically represented by a circular arrow icon (â†») or a trash/reset icon.\n",
    "\n",
    "**Run All** usually looks like a play button with multiple lines (â–¶â–¶) or says \"Run All\" as text.\n",
    "\n",
    "Look for the circular arrow or reset-style icon - that should be Clear. Does one of them look like that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a589e38",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "The tooltip only says restart, but it does clear. Is there a run all cells from here function?\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "I'm not aware of a built-in \"run all cells from here\" function in this Dialog Engineering environment. \n",
    "\n",
    "You'd typically need to either:\n",
    "1. Use \"Run All\" to run everything from the top\n",
    "2. Manually run cells one by one from your current position\n",
    "\n",
    "Some notebook interfaces have that feature, but I don't think this one does. You could try right-clicking on a cell to see if there's a context menu with more options?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b7992",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Thanks. Right clicking pops up a browser dialog. FYI\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Thanks for checking! Yeah, that's the browser's default context menu then - no special notebook features there.\n",
    "\n",
    "You're all set with your tool orchestration function. Is there anything else you'd like to work on, or are you finished with the assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54b3243",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "I am good. Now just cleaning up and deleting stuff of no value.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Sounds good! Cleaning up is always satisfying. Feel free to come back if you need help with anything else. Good luck with your assignment! ðŸ‘"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
